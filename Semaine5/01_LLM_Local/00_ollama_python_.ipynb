{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "778fbe1a",
   "metadata": {},
   "source": [
    "\n",
    "# Ollama x Python –\n",
    "\n",
    "Ce que vous allez faire :\n",
    "1. Vérifier que le serveur **Ollama** répond\n",
    "2. Lister vos modèles locaux\n",
    "3. Choisir un modèle (par défaut `llama3.1:8b`)\n",
    "4. Générer du texte avec `/api/generate`\n",
    "5. Discuter avec `/api/chat` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2489e390",
   "metadata": {},
   "source": [
    "## 0) Dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5e4381",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install requests -q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44d7e1b",
   "metadata": {},
   "source": [
    "## 1) Connexion serveur Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c806bd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statut HTTP : 200\n",
      "Version serveur : {'version': '0.6.2'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "\n",
    "r = requests.get(f\"{OLLAMA_HOST}/api/version\")\n",
    "print(\"Statut HTTP :\", r.status_code)\n",
    "print(\"Version serveur :\", r.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b469b9d7",
   "metadata": {},
   "source": [
    "## 2) Lister les modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90159254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèles locaux :\n",
      " - deepseek-r1:1.5b\n",
      " - llama3.1:8b\n",
      " - deepseek-r1:latest\n",
      " - llava:latest\n",
      " - mistral:latest\n",
      " - deepseek-r1:8b\n",
      " - llama3.2:latest\n",
      " - llama3.2:1b\n",
      " - deepseek-r1:7b\n"
     ]
    }
   ],
   "source": [
    "\n",
    "r = requests.get(f\"{OLLAMA_HOST}/api/tags\")\n",
    "data = r.json()\n",
    "models = [m[\"name\"] for m in data.get(\"models\", [])]\n",
    "print(\"Modèles locaux :\")\n",
    "for m in models:\n",
    "    print(\" -\", m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a99304",
   "metadata": {},
   "source": [
    "## 3) Choisir un modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c2e7d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle utilisé : llama3.1:8b\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MODEL = \"llama3.1:8b\"  # Changez si besoin, ex: \"deepseek-r1:8b\"\n",
    "print(\"Modèle utilisé :\", MODEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f872738",
   "metadata": {},
   "source": [
    "## 4) /api/generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ce3ea8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content-Type: application/json; charset=utf-8\n",
      "Un Large Language Model (LLM) est un modèle informatique conçu pour traiter du langage naturel, capable de comprendre et de générer du texte cohérent. Ce type de modèles utilisent des algorithmes de traitement automatique du langage (TAL) et d'apprentissage automatique profond pour traiter des données textuelles massives.\n",
      "\n",
      "Un cas d'usage consiste à utiliser un LLM en tant que support de rédaction, car il peut produire rapidement de la documentation, des contenus de blog ou même des articles complètement autonomes, ce qui facilite le travail des équipes de contenu.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "payload = {\n",
    "    \"model\": MODEL,\n",
    "    \"prompt\": \"Explique en 2 phrases ce qu'est un LLM et un cas d'usage.\",\n",
    "    \"stream\": False  # <--- important\n",
    "}\n",
    "r = requests.post(f\"{OLLAMA_HOST}/api/generate\", json=payload)\n",
    "print(\"Content-Type:\", r.headers.get(\"Content-Type\"))\n",
    "resp = r.json()  # OK car stream=False renvoie un seul JSON\n",
    "print(resp.get(\"response\", \"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d92e1",
   "metadata": {},
   "source": [
    "## 5) /api/chat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc5a2f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content-Type: application/json; charset=utf-8\n",
      "Voici trois bonnes pratiques pour écrire un prompt efficace :\n",
      "\n",
      "1. **Soyez clair et spécifique** : Formulez votre demande de manière précise et concrète, en évitant les ambiguïtés ou les termes vagues. Cela permettra à l'assistant d'apporter une réponse pertinente et adaptée.\n",
      "\n",
      "2. **Soyez précis dans vos objectifs** : Définissez clairement ce que vous voulez atteindre avec votre demande, en indiquant les résultats attendus ou les informations nécessaires. Cela facilitera l’entraînement de l’assistant à fournir une réponse optimale.\n",
      "\n",
      "3. **Restez concis et structuré** : Évitez les phrases longues et tortueuses pour formuler votre demande. Faites en sorte qu'elle soit facile à lire, claire et directe, ce qui rendra la tâche plus efficace pour l'assistant de fournir une réponse pertinente.\n",
      "\n",
      "Cela contribuera à améliorer les résultats obtenus avec l’assistant et facilitera votre travail.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Tu es un assistant concis et pédagogique.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Donne 3 bonnes pratiques pour écrire un prompt efficace.\"},\n",
    "]\n",
    "\n",
    "payload = {\n",
    "    \"model\": MODEL,\n",
    "    \"messages\": messages,\n",
    "    \"stream\": False  \n",
    "}\n",
    "r = requests.post(f\"{OLLAMA_HOST}/api/chat\", json=payload)\n",
    "print(\"Content-Type:\", r.headers.get(\"Content-Type\"))\n",
    "data = r.json()  # OK car stream=False\n",
    "print(data[\"message\"][\"content\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
