{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e15be5d6",
   "metadata": {},
   "source": [
    "\n",
    "# RAG PDF — Notebook d'essai **manuel** (sans Gradio)\n",
    "\n",
    "Ce notebook :\n",
    "1. Installe les dépendances Python requises  \n",
    "2. Vérifie la présence des binaires système pour l’OCR (Poppler/Tesseract)  \n",
    "3. Utilise vos **fonctions** dans le dossier `fonctions/` pour ingérer tous les PDF de `data/` (texte + tableaux + OCR)  \n",
    "4. Persiste la base vectorielle dans `.chroma_notebook/`  \n",
    "5. Permet des **tests manuels** (retrieval + LLM)  \n",
    "\n",
    "\n",
    "> **Important** : Le dossier `fonctions/` doit contenir : `config.py`, `embeddings.py`, `ingestion.py`, `retrieval.py`, `utils.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0574f8fa",
   "metadata": {},
   "source": [
    "## 0) Préparer l'environnement & vérifier le PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffad7e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Racine du projet: /Users/natachanjongwayepnga/Downloads/GENAI_PRO_Cohorte_1/Semaine5/01_RAG_GPT\n",
      "\n",
      "Contenu de la racine:\n",
      "- .DS_Store\n",
      "- .env\n",
      "- RAG_PDF_notebook.ipynb\n",
      "- README.md\n",
      "- app.py\n",
      "- constraints.txt\n",
      "- data\n",
      "- fonctions\n",
      "- requirements.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, pathlib, sys\n",
    "\n",
    "root = pathlib.Path(\".\").resolve()\n",
    "print(\"Racine du projet:\", root)\n",
    "\n",
    "# S'assurer que le dossier du projet est bien dans le PYTHONPATH\n",
    "if str(root) not in sys.path:\n",
    "    sys.path.append(str(root))\n",
    "\n",
    "print(\"\\nContenu de la racine:\")\n",
    "for p in sorted(root.glob(\"*\")):\n",
    "    print(\"-\", p.name)\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # ou \"true\" si tu veux le laisser activé\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c18a0b6",
   "metadata": {},
   "source": [
    "## 1) Installer les dépendances Python (exécuter une seule fois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb0f59e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -r requirements.txt -c constraints.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c72f5b4",
   "metadata": {},
   "source": [
    "## 2) Vérifier les binaires système pour l’OCR (Poppler / Tesseract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "051218b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdftoppm: OK → /opt/homebrew/bin/pdftoppm\n",
      "tesseract: OK → /opt/homebrew/bin/tesseract\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import shutil\n",
    "\n",
    "def check_bin(name):\n",
    "    path = shutil.which(name)\n",
    "    print(f\"{name}: {'OK → ' + path if path else 'NON TROUVÉ'}\")\n",
    "    return path\n",
    "\n",
    "poppler = check_bin(\"pdftoppm\")\n",
    "tess = check_bin(\"tesseract\")\n",
    "\n",
    "if not poppler:\n",
    "    print(\"\\n⚠️ Poppler manquant. Installez-le sur votre machine :\")\n",
    "    print(\"- macOS:  brew install poppler\")\n",
    "    print(\"- Ubuntu: sudo apt-get install poppler-utils\")\n",
    "\n",
    "if not tess:\n",
    "    print(\"\\n⚠️ Tesseract manquant. Installez-le sur votre machine :\")\n",
    "    print(\"- macOS:  brew install tesseract\")\n",
    "    print(\"- Ubuntu: sudo apt-get install tesseract-ocr\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f96ecb",
   "metadata": {},
   "source": [
    "## 3) Paramètres/Environnement (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fa5c7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR attendu : /Users/natachanjongwayepnga/Downloads/GENAI_PRO_Cohorte_1/Semaine5/01_RAG_GPT/data\n",
      "Vector store (notebook) : /Users/natachanjongwayepnga/Downloads/GENAI_PRO_Cohorte_1/Semaine5/01_RAG_GPT/.chroma\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# (Optionnel) Clé OpenAI pour utiliser un LLM OpenAI dans le notebook\n",
    "# Si vous laissez vide, la cellule LLM échouera — c'est normal ; vous pouvez tester uniquement le retrieval.\n",
    "os.environ.setdefault(\"OPENAI_API_KEY\", \"\")  # <-- Renseignez votre clé si vous souhaitez tester le LLM ici\n",
    "\n",
    "print(\"DATA_DIR attendu :\", Path(\"data\").resolve())\n",
    "print(\"Vector store (notebook) :\", Path(\".chroma\").resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f4de3c",
   "metadata": {},
   "source": [
    "## 4) Importer les fonctions depuis `fonctions/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f61b4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "- DATA_DIR        : /Users/natachanjongwayepnga/Downloads/GENAI_PRO_Cohorte_1/Semaine5/01_RAG_GPT/data\n",
      "- PERSIST_DIR     : .chroma\n",
      "- COLLECTION_NAME : pdf_collection\n",
      "- TOP_K           : 8\n",
      "- ENABLE_OCR      : True\n",
      "- OCR_MIN_TEXT_CHARS: 120\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from fonctions.config import DATA_DIR, PERSIST_DIR, COLLECTION_NAME, TOP_K, ENABLE_OCR, OCR_MIN_TEXT_CHARS\n",
    "from fonctions.ingestion import ingest_all_pdfs\n",
    "from fonctions.retrieval import get_vectorstore, retrieve_docs\n",
    "\n",
    "print(\"Config:\")\n",
    "print(\"- DATA_DIR        :\", DATA_DIR)\n",
    "print(\"- PERSIST_DIR     :\", PERSIST_DIR)\n",
    "print(\"- COLLECTION_NAME :\", COLLECTION_NAME)\n",
    "print(\"- TOP_K           :\", TOP_K)\n",
    "print(\"- ENABLE_OCR      :\", ENABLE_OCR)\n",
    "print(\"- OCR_MIN_TEXT_CHARS:\", OCR_MIN_TEXT_CHARS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde91f49",
   "metadata": {},
   "source": [
    "## 5) Ingestion de tous les PDFs (`data/*.pdf`) & persistance dans `.chroma_notebook/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "879c8a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ingestion terminée et base vectorielle persistée dans .chroma\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  Placez vos PDF dans le dossier data/ avant d’exécuter cette cellule\n",
    "vs = ingest_all_pdfs()\n",
    "print(\"✅ Ingestion terminée et base vectorielle persistée dans\", PERSIST_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaaac91",
   "metadata": {},
   "source": [
    "## 6) (Re)charger la base persistée (si redémarrage du kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bef82afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store chargé : True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vs = get_vectorstore()\n",
    "print(\"Vector store chargé :\", vs is not None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998b5460",
   "metadata": {},
   "source": [
    "## 7) RAG minimal (retrieval MMR + LLM). **Note** : nécessite `OPENAI_API_KEY` si vous voulez poser des questions au LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6097417",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def _normalize_score(s: float) -> float:\n",
    "    if s < 0:      # cosine [-1,1]\n",
    "        s = (s + 1.0) / 2.0\n",
    "    elif s > 1:    # distance\n",
    "        s = 1.0 / (1.0 + s)\n",
    "    return max(0.0, min(1.0, s))\n",
    "\n",
    "def _load_llm():\n",
    "    key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "    if not key:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY manquante. Renseignez-la pour tester le LLM dans ce notebook.\")\n",
    "    return ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "\n",
    "PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Tu es un assistant factuel. Réponds en français UNIQUEMENT avec le contexte fourni. Cite les pages: [p. X].\"),\n",
    "    (\"user\", \"\"\"Question:\n",
    "{question}\n",
    "\n",
    "Contexte:\n",
    "{context}\n",
    "\n",
    "Règles:\n",
    "- Sois concis et précis\n",
    "- Termine par \"Sources: ...\" avec les pages\n",
    "\"\"\")\n",
    "])\n",
    "PARSER = StrOutputParser()\n",
    "LLM = None\n",
    "\n",
    "def _format_context(hits):\n",
    "    def fmt_piece(doc):\n",
    "        txt = doc.page_content or \"\"\n",
    "        typ = (doc.metadata or {}).get(\"type\", \"text\")\n",
    "        # pas de troncature pour les tables/table_flat\n",
    "        if typ not in (\"table\", \"table_flat\") and len(txt) > 1600:\n",
    "            txt = txt[:1600] + \" ...\"\n",
    "        return f\"(p. {doc.metadata.get('page')}) {txt}\"\n",
    "    parts = [fmt_piece(d) for d, _ in hits]\n",
    "    pages = sorted({(d.metadata or {}).get(\"page\") for d, _ in hits if (d.metadata or {}).get(\"page\") is not None})\n",
    "    return \"\\n\\n\".join(parts), pages\n",
    "\n",
    "def ask(question: str, k: int = 8):\n",
    "    try:\n",
    "        raw = vs.max_marginal_relevance_search_with_score(\n",
    "            question, k=k, fetch_k=max(40, k*8), lambda_mult=0.1\n",
    "        )\n",
    "    except Exception:\n",
    "        raw = vs.similarity_search_with_relevance_scores(question, k=k)\n",
    "    hits = [(doc, _normalize_score(score)) for doc, score in raw]\n",
    "    if not hits:\n",
    "        return \"Aucun passage pertinent trouvé.\", []\n",
    "    context, pages = _format_context(hits)\n",
    "    global LLM\n",
    "    if LLM is None:\n",
    "        LLM = _load_llm()\n",
    "    answer = (PROMPT | LLM | PARSER).invoke({\"question\": question, \"context\": context})\n",
    "    if pages:\n",
    "        answer += \"\\n\\nSources: \" + \", \".join([f\"p. {p}\" for p in pages])\n",
    "    return answer, hits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7cdda6",
   "metadata": {},
   "source": [
    "## 8) Essai manuel — posez une question au LLM (contexte = vos PDFs ingérés)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e339b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le chiffre d'affaires total de Tesla au T1 2023 s'élève à 23,3 milliards de dollars, ce qui représente une croissance de 24 % par rapport à l'année précédente [p. 4]. \n",
      "\n",
      "Sources: [p. 4], [p. 22].\n",
      "\n",
      "Sources: p. 2, p. 4, p. 5, p. 8, p. 18, p. 19, p. 22, p. 27\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"Quel est le total revenue (chiffre d’affaires) de Tesla au T1 2023 ?\"\n",
    "reponse, hits = ask(question, k=8)\n",
    "print(reponse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57df0fc5",
   "metadata": {},
   "source": [
    "## 9)  Inspecter les passages renvoyés par le retrieval (sans LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cececc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[p.6] score=0.314 type=text :: Source: Tesla estimates based on ACEA; Autonews.com; CAAM – light-duty vehicles only TTM = Trailing twelve months7 In Q1, we produced a record number of vehicles, thanks to ongoing ramps at our  factories in Austin and Berlin. We remain com …\n",
      "[p.5] score=0.295 type=text :: Q1-2022 Q2-2022 Q3-2022 Q4-2022 Q1-2023 YoY Model S/X production 14,218 16,411 19,935 20,613 19,437 37% Model 3/Y production 291,189 242,169 345,988 419,088 421,371 45% Total production 305,407 258,580 365,923 439,701 440,808 44% Model S/X  …\n",
      "[p.18] score=0.260 type=text :: 0 1 2 3 4 5 6 2Q-2020 3Q-2020 4Q-2020 1Q-2021 2Q-2021 3Q-2021 4Q-2021 1Q-2022 2Q-2022 3Q-2022 4Q-2022 1Q-2023 0 1 2 3 4 5 6 2Q-2020 3Q-2020 4Q-2020 1Q-2021 2Q-2021 3Q-2021 4Q-2021 1Q-2022 2Q-2022 3Q-2022 4Q-2022 1Q-2023 0.0 0.1 0.2 0.3 0.4  …\n",
      "[p.19] score=0.246 type=text :: 0 2 4 6 8 10 12 14 16 18 20 2Q-2020 3Q-2020 4Q-2020 1Q-2021 2Q-2021 3Q-2021 4Q-2021 1Q-2022 2Q-2022 3Q-2022 4Q-2022 1Q-2023 0 2 4 6 8 10 12 14 16 18 20 2Q-2020 3Q-2020 4Q-2020 1Q-2021 2Q-2021 3Q-2021 4Q-2021 1Q-2022 2Q-2022 3Q-2022 4Q-2022  …\n",
      "[p.6] score=0.241 type=text :: V E H I C L E C A P A C I T Y Installed Annual Vehicle Capacity Region Model Capacity Status California Model S / Model X 100,000 Production Model 3 / Model Y 550,000 Production Shanghai Model 3 / Model Y >750,000 Production Berlin Model Y  …\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"Combien de véhicules ont été livrés au T1 2023 ?\"\n",
    "topk = 10\n",
    "pairs = retrieve_docs(vs, question, k=topk)\n",
    "for d, sc in pairs[:5]:\n",
    "    typ = (d.metadata or {}).get(\"type\")\n",
    "    pg = (d.metadata or {}).get(\"page\")\n",
    "    preview = (d.page_content or \"\").replace(\"\\n\", \" \")[:240]\n",
    "    print(f\"[p.{pg}] score={sc:.3f} type={typ} :: {preview} …\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-tsla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
