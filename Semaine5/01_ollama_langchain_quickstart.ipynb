{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd734413",
   "metadata": {},
   "source": [
    "\n",
    "# Utiliser Ollama avec LangChain \n",
    "\n",
    "\n",
    "Ce notebook montre comment :\n",
    "- Configurer LangChain pour utiliser un modèle Ollama local\n",
    "- Faire des requêtes simples\n",
    "- Utiliser le mode conversationnel avec mémoire\n",
    "- Chaîner plusieurs étapes (prompt templates, parsing structuré, etc.)\n",
    "- Générer des embeddings pour la recherche sémantique\n",
    "\n",
    "> Prérequis : Avoir installé et lancé [Ollama](https://ollama.com/) et installé `langchain` & `langchain-community`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1f6ad5",
   "metadata": {},
   "source": [
    "## 1) Installation des packages nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0da6ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install langchain langchain-community requests\n",
    "# Assurez-vous que Ollama est installé et lancé : ollama serve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44fa909",
   "metadata": {},
   "source": [
    "## 2) Import et configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85e1ee5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle choisi : llama3.1:8b\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Choix du modèle\n",
    "MODEL_PREFERENCE = [\n",
    "    \"llama3.1:8b\",\n",
    "    \"deepseek-r1:8b\",\n",
    "    \"mistral:latest\",\n",
    "    \"deepseek-r1:7b\",\n",
    "    \"deepseek-r1:latest\"\n",
    "]\n",
    "\n",
    "# ⚠️ Mettez le modèle que vous voulez utiliser en priorité ici si vous le connaissez\n",
    "chosen_model = MODEL_PREFERENCE[0]\n",
    "\n",
    "print(\"Modèle choisi :\", chosen_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24f2e42",
   "metadata": {},
   "source": [
    "## 3) Utilisation simple d'un LLM avec LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09c1f5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_h/9jm4l3w973n8rgwzstgvzkzc0000gn/T/ipykernel_2392/96196775.py:1: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=chosen_model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voici trois points qui expliquent pourquoi LangChain peut être utile dans le domaine de l'intelligence artificielle générative :\n",
      "\n",
      "1. **Modélisation et gestion des chaînes de dialogue** : LangChain fournit une infrastructure pour la modélisation et la gestion des chaînes de dialogue, ce qui est crucial pour les applications d'IA générative telles que le chatbots ou les assistants virtuels. Cela permet à l'IA de naviguer dans des conversations complexes et d'interagir avec l'utilisateur d'une manière plus naturelle.\n",
      "\n",
      "2. **Intégration de plusieurs modèles IA** : LangChain facilite l'intégration de multiples modèles IA, ce qui est particulièrement utile pour les tâches qui nécessitent une combinaison de capacités cognitives différentes, telles que la reconnaissance vocale et la génération de texte. Cela permet à l'IA d'accéder à un large éventail de compétences et de générer des résultats plus polyvalents.\n",
      "\n",
      "3. **Réutilisation et recombinaison de modèles** : LangChain offre la possibilité de réutiliser et combiner des modèles IA pour créer de nouvelles capacités, ce qui est essentiel dans le domaine de l'IA générative où les tâches sont souvent complexes et nécessitent une approche flexible. Cela permet aux développeurs d'enrichir leurs applications existantes tout en minimisant les coûts et la complexité.\n",
      "\n",
      "En résumé, LangChain offre des fonctionnalités qui facilitent la modélisation, l'intégration et la combinaison de modèles IA génératifs, ce qui permet aux développeurs d'appliquer ces capacités dans divers domaines.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm = Ollama(model=chosen_model)\n",
    "\n",
    "prompt = \"Explique en 3 points pourquoi LangChain est utile pour l'IA générative.\"\n",
    "response = llm.invoke(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275c81e2",
   "metadata": {},
   "source": [
    "## 4) Mode conversationnel avec mémoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7bcdb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_h/9jm4l3w973n8rgwzstgvzkzc0000gn/T/ipykernel_2392/2059108945.py:4: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  chat_llm = ChatOllama(model=chosen_model, temperature=0.7)\n",
      "/var/folders/_h/9jm4l3w973n8rgwzstgvzkzc0000gn/T/ipykernel_2392/2059108945.py:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "/var/folders/_h/9jm4l3w973n8rgwzstgvzkzc0000gn/T/ipykernel_2392/2059108945.py:7: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Bonjour, je suis un développeur Python et je veux apprendre LangChain.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Bonjour, je suis un développeur Python et je veux apprendre LangChain.\n",
      "AI: Bonjour ! Je suis ravi de faire votre connaissance. LangChain est une bibliothèque très intéressante pour la recherche d'information et le traitement du langage naturel. Elle permet notamment de créer des modèles de chatbots et de chatbots avancés capables de résoudre des problèmes complexes.\n",
      "\n",
      "Je vous conseille de commencer par lire la documentation officielle, qui explique bien les fonctionnalités et les utilisations possibles de LangChain. Vous pouvez également regarder les exemples de code fournis pour voir comment ils utilisent différentes fonctionnalités de la bibliothèque.\n",
      "\n",
      "LangChain est basée sur des techniques de traitement du langage naturel telles que le tokenization, la part-of-speech tagging et la détection d'entités nommées. Elle utilise également des algorithmes tels que les réseaux neuronaux pour effectuer des tâches telles que la classification textuelle et la génération de texte.\n",
      "\n",
      "Si vous avez des questions spécifiques sur LangChain, je suis là pour vous aider ! Qu'est-ce que vous voulez savoir en premier ?\n",
      "Human: Donne-moi maintenant 2 cas d'usage concrets pour la finance.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Les utilisations de LangChain dans le domaine financier sont vastes et prometteuses. Voici deux exemples de cas d'utilisation concrets :\\n\\n1. **Analyse sentimentale des rapports financiers** : avec LangChain, vous pouvez créer un modèle capable d'analyser les rapports financiers des entreprises et d'extraire l'information pertinente. Par exemple, le modèle peut détecter les mentions de risques ou opportunités financières, identifier les tendances dans les résultats financiers et même évaluer la santé financière de l'entreprise. Vous pouvez également utiliser LangChain pour générer des résumés synthétiques de ces rapports, facilitant ainsi la compréhension pour les investisseurs ou les analystes.\\n\\n2. **Génération automatique de notes d'information sur les actions** : LangChain permettrait la création d'un système qui peut analyser les données financières et des marchés pour générer des notes d'information précises sur les actions. Ces notes peuvent inclure des analyses techniques, des recommandations d'achat ou de vente, ainsi que des perspectives à long terme. Ce type d'outil pourrait être très utile pour les traders individuels ou même pour les équipes de trading dans les grandes banques d'affaires.\\n\\nCes exemples démontrent l'ampleur de ce qu'on peut accomplir avec LangChain en finance ! Qu'est-ce que vous pensez de ces idées ? Avez-vous des questions sur comment mettre en œuvre ces projets ou souhaitez-vous explorer d'autres utilisations potentielles de LangChain dans le domaine financier ?\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chat_llm = ChatOllama(model=chosen_model, temperature=0.7)\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=chat_llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Bonjour, je suis un développeur Python et je veux apprendre LangChain.\")\n",
    "conversation.predict(input=\"Donne-moi maintenant 2 cas d'usage concrets pour la finance.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953ccf1c",
   "metadata": {},
   "source": [
    "## 5) Utiliser un PromptTemplate dans une chaîne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "599a0da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_h/9jm4l3w973n8rgwzstgvzkzc0000gn/T/ipykernel_2392/919777948.py:12: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt_template)\n",
      "/var/folders/_h/9jm4l3w973n8rgwzstgvzkzc0000gn/T/ipykernel_2392/919777948.py:15: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  summary = chain.run(text=text_to_summarize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voici une résumé du texte en trois phrases maximum :\n",
      "\n",
      "LangChain est une bibliothèque Python. Elle permet la création d'applications avec des LLMs (Language Models). Cela signifie qu'elle facilite l'utilisation de ces modèles pour les applications.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "template = \"\"\"\n",
    "Tu es un expert en analyse de texte.\n",
    "Résume le texte suivant en 3 phrases maximum :\n",
    "\n",
    "Texte : {text}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(input_variables=[\"text\"], template=template)\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "text_to_summarize = \"LangChain est une bibliothèque Python qui facilite la création d'applications utilisant des LLMs.\"\n",
    "summary = chain.run(text=text_to_summarize)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0883b608",
   "metadata": {},
   "source": [
    "## 6) Sortie structurée (parsing JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92070ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'titre': 'LangChain : bibliothèque Python pour les LLMs', 'puces': [\"Facilite la création d'applications utilisant des LLMs\", 'Bibliothèque Python', 'LLMs (Large Language Models) supportés']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "schemas = [\n",
    "    ResponseSchema(name=\"titre\", description=\"Titre du résumé\"),\n",
    "    ResponseSchema(name=\"puces\", description=\"Liste de 3 puces clés\")\n",
    "]\n",
    "\n",
    "parser = StructuredOutputParser.from_response_schemas(schemas)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "prompt_structured = f\"\"\"\n",
    "Résume le texte suivant et retourne les résultats au format JSON.\n",
    "\n",
    "Texte : {text_to_summarize}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "structured_output = llm.invoke(prompt_structured)\n",
    "try:\n",
    "    parsed = parser.parse(structured_output)\n",
    "    print(parsed)\n",
    "except Exception as e:\n",
    "    print(\"Erreur de parsing :\", e)\n",
    "    print(\"Sortie brute :\", structured_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24e70fa",
   "metadata": {},
   "source": [
    "## 7) Génération d'embeddings avec LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f44e8cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_h/9jm4l3w973n8rgwzstgvzkzc0000gn/T/ipykernel_2392/4018311019.py:1: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=chosen_model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension : 4096\n",
      "Aperçu : [-1.2082576751708984, -2.6204404830932617, 1.5742257833480835, 2.3475940227508545, -0.6658380627632141, -2.4030861854553223, -0.442448228597641, 1.4911141395568848] ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embeddings = OllamaEmbeddings(model=chosen_model)\n",
    "vector = embeddings.embed_query(\"Les modèles de langage facilitent l'automatisation.\")\n",
    "print(\"Dimension :\", len(vector))\n",
    "print(\"Aperçu :\", vector[:8], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c5a283",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "Vous pouvez maintenant utiliser LangChain avec vos modèles Ollama pour :\n",
    "- Faire des requêtes simples et conversations avec mémoire\n",
    "- Structurer vos prompts et obtenir des sorties JSON\n",
    "- Générer des embeddings pour la recherche sémantique\n",
    "\n",
    "Prochaine étape : intégrer ces chaînes dans des pipelines plus complexes (RAG, agents, etc.).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
